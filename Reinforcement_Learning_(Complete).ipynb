{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning (Complete).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harrow-Enigma/ai-lecture-series-summer21/blob/main/Reinforcement_Learning_(Complete).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW9Sb0P6mOI-"
      },
      "source": [
        "# Reinforcement Learning\n",
        "\n",
        "Make an agent that plays cartpole!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqw--Trz1MKg"
      },
      "source": [
        "Copyright 2021 Team Enigma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "XiP31wv21QpV"
      },
      "source": [
        "# Copyright 2021 Team Enigma\n",
        "\n",
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q2IW9gIa6pG"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import gym\n",
        "import pickle as pkl\n",
        "import time, os\n",
        "import numpy as np\n",
        "from numpy.random import choice as sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id7uFYgAR9xY"
      },
      "source": [
        "## Make the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxhqm6SgiQPH"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "ACTION_SPACE = env.action_space\n",
        "OBSERVATION_SPACE = env.observation_space\n",
        "\n",
        "print('Action space: ', env.action_space.n)\n",
        "print('Observation space: ', env.observation_space.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtuFxl3FR_rg"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgJx5I8I6p9e"
      },
      "source": [
        "def normalize(arr):\n",
        "  arr = np.asarray(arr, dtype=np.float32)\n",
        "  mean = arr.mean()\n",
        "  std = arr.std()\n",
        "  ret = (arr - mean) / std\n",
        "  return ret.astype(np.float32)\n",
        "\n",
        "def discounted_rewards(r, gamma=0.95):\n",
        "  dr = np.zeros_like(r, dtype=np.float32)\n",
        "  R = 0\n",
        "  for i in reversed(range(len(r))):\n",
        "    R = R * gamma + r[i]\n",
        "    dr[i] = R\n",
        "  return normalize(dr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nklq_6imhGXZ"
      },
      "source": [
        "class History(object):\n",
        "  def __init__(self):\n",
        "    self.rewards=[]\n",
        "    self.observations=[]\n",
        "    self.actions=[]\n",
        "  \n",
        "  def restart(self):\n",
        "    self.rewards=[]\n",
        "    self.observations=[]\n",
        "    self.actions=[]\n",
        "  \n",
        "  def write(self,observation,action,reward):\n",
        "    self.rewards.append(reward)\n",
        "    self.observations.append(observation)\n",
        "    self.actions.append(action)\n",
        "  \n",
        "  def solidify(self):\n",
        "    self.rewards=np.array(self.rewards)\n",
        "    self.observations=np.array(self.observations)\n",
        "    self.actions=np.array(self.actions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5pLQ1EPSCt1"
      },
      "source": [
        "## Keras model for our agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARlsDwWj1iJa"
      },
      "source": [
        "class Agent(keras.Model):\n",
        "    def __init__(self, action_space):\n",
        "        super(Agent, self).__init__()\n",
        "        self.output_dim = action_space\n",
        "        self.dense0 = keras.layers.Dense(100, activation='relu')\n",
        "        self.dense1 = keras.layers.Dense(100, activation='relu')\n",
        "        self.dense2 = keras.layers.Dense(self.output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense0(inputs)\n",
        "        x = self.dense1(x)\n",
        "        return self.dense2(x)\n",
        "    \n",
        "    def act(self, observations):\n",
        "      self.observations = np.expand_dims(observations,axis=0)\n",
        "      self.raw = self.predict(self.observations)\n",
        "      self.raw = tf.nn.softmax(self.raw)\n",
        "      self.out = np.squeeze(self.raw,0)\n",
        "      self.action = np.random.choice(self.output_dim,1,p=self.out)\n",
        "      return self.action[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o28JFwbT0-5D"
      },
      "source": [
        "# Initialising model\n",
        "agent = Agent(ACTION_SPACE.n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUukvxZ82Eli"
      },
      "source": [
        "# Pass sample information\n",
        "obs = env.reset()\n",
        "agent.act(obs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aYKTCY-SKCO"
      },
      "source": [
        "## Training Steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFXuZ0zvq2r8"
      },
      "source": [
        "# Policy optimization loss\n",
        "def loss(actions, logits, rewards):\n",
        "  neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(actions,\n",
        "                                                                logits)\n",
        "  return tf.reduce_mean(neg_log_prob * rewards)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKZM5K_URAjY"
      },
      "source": [
        "# Custom optimizer\n",
        "optimizer=tf.keras.optimizers.Adam(1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVRuzPWTqapv"
      },
      "source": [
        "# One training step - replaying from memory\n",
        "def train_step(history, agent):\n",
        "  r=discounted_rewards(history.rewards)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = agent(history.observations)\n",
        "    losses = loss(history.actions,logits,r)\n",
        "\n",
        "  gradients = tape.gradient(losses, agent.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, agent.trainable_variables))\n",
        "  return tf.reduce_sum(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sierQH2gSWnR"
      },
      "source": [
        "## Training!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unTgo4GPrI2V"
      },
      "source": [
        "history = History()\n",
        "\n",
        "for i_episode in range(500):\n",
        "    print('Starting edpisode {}'.format(i_episode))\n",
        "\n",
        "    observation = env.reset()\n",
        "    history.restart()\n",
        "    t = 0\n",
        "\n",
        "    while True:\n",
        "      action = agent.act(observation)\n",
        "      obs, reward, done, info = env.step(action)\n",
        "      history.write(observation, action, reward)\n",
        "      t += 1\n",
        "      observation = obs\n",
        "\n",
        "      if done:\n",
        "          history.solidify()\n",
        "          losses = train_step(history, agent)\n",
        "          print(\"Episode finished after {} timesteps, with a loss of {}\\n\".format(t,losses))\n",
        "          break\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpkBAVXtKlE_"
      },
      "source": [
        "agent.save_weights('weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}